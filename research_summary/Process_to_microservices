A typical server side enterprise application has either layered or hexagonal architecture and consists of various components such as :
Presentation components
Business logic
Database access logic
Application integration logic
Additionally, it has to support various types of clients such as browser, mobile browsers, native applications and 3rd party applications. 

This can be achieved by applying scale cube to the application architecture. First apply y-axis scaling (functional decomposition) into a set of collaborating components, each component fulfilling a part of small business functions. Each component are then scaled along X-axis (cloning) and then z-axis(partitionaling database).[1]

How do we chose the individual functional components?
1. by verb or usecase
2. by noun or resources : determine grouping operations around a single resource.
3. using Single Responsibility Principle: each service should have only small set of responsibilities adhering to SRP which mentions that each class should have only one reason to change and maps responsibility of a class as a reason to change.
4. by following the analogy to design of Unix utilities. each unix does exactly one thing well and can be combined with other utilities. We can select the functionalities which do not provide business value by themselves but are used by services such as loging service etc.[1]
5. Another most common technique used is Domain driven design. A business domain is divided into a set of business capabilities, each business capability is then mapped into a set of bounded contexts. Each bounded context fulfills a single business capability and encapsulates data related to the capability. Each bounded context has its own view of the domain model and only deals with the attributes it finds interesting.[2]
Neuro-linguistic programming provides a technique called chunking, which can be used to derive levels of abstraction way up from individual object towards business capability and thus defining the bounded context.  Microservice perforoms a single task around a specific business capability and within a single distributed bounded context. [3]

Unpacking the domain
Neuro-linguistic programming can be used to dig deep into domain and uncover various layers of abstraction while following Single Responsibility principle.
Consider following user story for log in.
    Story: User logs in
    Scenario: User with valid credentials
    Given an unauthenticated user
    When the user tries to navigate to the welcome page
    Then they should be redirected to the login page
    When the user enters a valid name in the Name field
    And the user enters the corresponding password in the Password field
    And the user presses the Login button
    Then they should be directed to the welcome page
    
When we see this scenario and analyze, It has terms from different domains such as authentication, UI and web. Each domain has a specific stakeholder. So, if we develop a single application based on this scenario, there are more than one stakeholder affecting this scenario and thus more than one reason to change or break.
In order to restrict the focus of the scenario, we can use Neuro-linguistic programming. Firstly we ash 'what for' question to the scenario. The clear answer is user authentication. It can be a wholesome domain for the scenario and has a single stakeholder. In order to formulate further layers of abstractions, We as 'how'. We get following scenario. Finally, at each abstraction, we achieve single responsiblity small enough for the layer. [4]

    Scenario: User with valid credentials

    Given an unauthenticated user
    When the user tries to access a restricted asset
    Then they should be denied the access.
    When the user provides valid credentials
    Then they should be provided with the content.
    
Here, the scenario description again speaks only within a single domain. However, each scenario description points does not answer the details of 'how' but only merely abstraction of 'what' of 'how' of the layer above. So, any laver is what of successor layer and how of the predecessor layer. In this way we can divide a domain into various layers of subdomains. [4]

Reusability and loose coupling are among the goals of service orientation. However if we make microservices very small to increase reusability then we tend to compromise loose coupling. The size of microservice should be optimum to balance both reusability and loose coupling. The coupling becomes catostrophic if we use synchronous communication in such finely grained microservices. So, in order determine the size we also need to consider 8 fallacies of distributed computing. At this point it is important to mention an example for creating a legal entity which also creates an address. We should not only consider the usecase to create microservices but also how the microservices will be used to create business value.
We should also consider the rule, 1 usecase = 1 transaction = 1 aggregate
The way data is changed or used determines the boundary. Since, address is created everytime a legal entity is created, we need to put these into a single entity or aggregate. Another reason is that the existing relation between legal entity and Address is parent-child relationship, meaning that both are updated, created at the same time. So, they should be inside same boundary.
By doing so, we apply tactical design of DDD and give authority to a single entity and maintain consistency within the boundary. By doing so we make sure that there is only one authority to the data and since we are creating boundary around business functionality, the rules as well as the data which the rules apply resides within the boundary. This also means that there will be single reason to change and comply Single Responsibility principle. 
Applying bounded context, consistency rule helps to define clear boundary between services (or divide data/services) and perserves autonomy. It will be better to look in to an example of boundaries sales order and Product and what happens when we want to add order confirmation usecase. This usecase can be achieved by implementing the usecase in Sales order by duplicating the product. This way it some how loosely follow SRP but provides autonomy. The other way is to create a different microservices and duplicate product there. This will follow autonomy as well as SRP and will be easy to replace. One way communication (events) can be used to ensure autonomy. If we create services around business capabilities rather than technical capabilities then we can achieve single reason to change each service and which is the business usecase it is responsible for.[5]

"
With coherent I’m thinking about both communicational cohesion (logic and the data the logic is working on are both part of the same service), layered cohesion (a service is responsible for data persistence, handling business-related-security*, user interface, etc.) and Temporal cohesion (the parts / aspects involved in the handling services functionality are grouped together so that they can be executed close to each other in time). Without communicational cohesion and layered cohesion, it is impossible to achieve Temporal cohesion which is a prerequisite for the Second SOA Principle “Services are autonomous.”
" [6]
    



drawbacks:
Increased Memory consumption: "The microservices architecture replaces N monolithic application instances with NxM services instances. If each service runs in its own JVM (or equivalent), which is usually necessary to isolate the instances, then there is the overhead of M times as many JVM runtimes. Moreover, if each service runs on its own VM (e.g. EC2 instance), as is the case at Netflix, the overhead is even higher."  ---- need to find out for deployment and scaling of microservices [1]



\\Evaluating Service Identification with Design Metrics on Business Process DecompositionThe designers attempt to arrive at a better partitioning of business processes in terms of their subjective comprehension on general principles for various features, such as services should be identified to satisfy loose coupling and high cohesion [10]the identification of appropriate services is a multiple objectives optimization problem [7]. On the one hand, service designers need to have a whole picture on the partitioning of business processes in order to evaluate the features of the identified services, such as granularity, coupling and cohesion, etc. On the other hand, they have to make a tradeoff among the various features which are mutually exclusive. For instance, a general principle is to design coarse-grained services to avoid the fine-grained interactions. However, the coarse-grained services usuallyhave low cohesion since the large scope of functionality makes them not focus on a single task. Moreover, intuitively, the coarse-grained services reduce couplingsince fewer messages are needed to be exchanged across them. However, it is not always true Therefore, most of researches (e.g. [4]) suggest that the identified services should achieve loose coupling by reducing connections between services, eliminating unnecessary relationships, and transforming the dependencies between them to few, well-known dependencies.High cohesion is a well-known preferred feature of service design, because it increases the clarity and ease of comprehension of identified services, thereby simplifying maintenance and future enhancements [10]. A service grouping low cohesive activities leads to a large number of unrelated invocations to access it, which enlarges the ripple effect of changes on the service. In addition, encapsulating tightly related activities into one service will benefit its reusability in different contexts due to clearer application domain of the service.Business entity convergence, we called in this paper, represents another consideration on service identification, i.e. the extent to which a service focuses on processing specific business entities.Taking the illustration scenario as an example, it is natural to group business activities Create Work Order and Update Work Order to one service since both of them operate the business entity Work Order. Based on this consideration, we argue that a service portfolio with better business entity convergence should satisfy: 1) a service is intended to be identified to focus on processing business entities as few as possible; 2) activities processing the same entities are preferred to be partitioned into the same service. To measure 1), we use the average number of business entities processed by a service. For measuring 2), we use the average number of services that one business entity spread out.In comparison with identifying services purely from the activity flows, the analysis of business entities forms an information-centric viewpoint for the service-based processes decomposition [9]. Entity-based and activity flow-based service identifications are two complementary techniques. Relying on a single viewpoint tends to construct a non-optimal service portfolio [1]. It is possible that there are no strong relationships among the activities acting on the same business entities from the observation of activity flows. However, grouping them together may demonstrate high value to business [6]. For instance, the business activity Update Work Order in the scenario is to save all the updates of a work order. If we do not care the business entity convergence and merely optimize from activity flows, the system may re-implement the updating operation in multiple services, which is an insufficient approach obviously. Furthermore, a service encapsulating all the actions on a business entity is preferred to be highly reused because of its clear functionality.Service identification complexity not only comes from multiple features of services we need to consider, but also a tradeoff that needs to be achieved since the features are mutually exclusive. As discussed in Section 2.2, when we increase the granularity, the cohesion of a service goes down due to not focusing on a single conceptual task.  Although we can expect that the coupling becomes looser because more data flows are potentially within services, it is not always true since the determinant is still the messages across the services. We need to ensure that the activities with extensive data transmissions are grouped to one service.Business entity convergence is a different perspective to do a service-based partitioning. However, when a service has better entity convergence, it may lead to the service receiving lots of messages for processing business entities. It is also relatively fine-grained since it is expected to work on limited business entities.
References:
1.http://microservices.io/patterns/microservices.html
2.http://www.mattstine.com/2014/06/30/microservices-are-solid
3.http://bovon.org/archives/350
4.http://dannorth.net/2011/01/31/whose-domain-is-it-anyway/
5.https://www.tigerteam.dk/2014/microservices-its-not-only-the-size-that-matters-its-also-how-you-use-them-part-4/
6.https://www.tigerteam.dk/2014/soa-synchronous-communication-data-ownership-and-coupling/

